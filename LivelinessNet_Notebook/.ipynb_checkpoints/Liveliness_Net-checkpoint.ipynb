{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liveliness Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will build a Liveliness Detection system.\n",
    "\n",
    "Face recognition systems are becoming more prevalent than ever. From face recognition on your smartphone, to face recognition for mass surveillance in China, face recognition systems are being utilized everywhere.\n",
    "\n",
    "However, face recognition systems are easily fooled by “spoofing” and “non-real” faces.\n",
    "\n",
    "Face recognition systems can be circumvented simply by holding up a photo of a person (whether printed, on a smartphone, etc.) to the face recognition camera.\n",
    "\n",
    "In order to make face recognition systems more secure, we need to be able to detect such fake/non-real faces — liveness detection is the term used to refer to such algorithms.\n",
    "\n",
    "There are a number of approaches to liveness detection, including:\n",
    "   - **Texture analysis**: including computing Local Binary Patterns (LBPs) over face regions and using an SVM to classify the faces as real or spoofed.\n",
    "   - **Frequency analysis**: such as examining the Fourier domain of the face.\n",
    "   - **Variable focusing analysis**: such as examining the variation of pixel values between two consecutive frames.\n",
    "   - **Heuristic-based algorithms**: including eye movement, lip movement, and blink detection. These set of algorithms attempt to track eye movement and blinks to ensure the user is not holding up a photo of another person (since a photo will not blink or move its lips).\n",
    "   - **Optical Flow algorithms**: namely examining the differences and properties of optical flow generated from 3D objects and 2D planes.\n",
    "   - **3D face shape**: similar to what is used on Apple’s iPhone face recognition system, enabling the face recognition system to distinguish between real faces and printouts/photos/images of another person.\n",
    "   - **Combinations of the above**: enabling a face recognition system engineer to pick and choose the liveness detections models appropriate for their particular application.\n",
    "\n",
    "A full review of liveness detection algorithms can be found in [Chakraborty and Das’ 2014 paper, An Overview of Face liveness Detection](https://arxiv.org/pdf/1405.2227.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook:** \n",
    "\n",
    "- We will be treating liveness detection using **Heuristic-based algorithms**. we will use build blink detection based on a binary classification of the eye status (open/closed). \n",
    "- We choose [leNet-5 model](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).\n",
    "- Train our model on [Closed Eyes In The Wild (CEW) dataset](http://parnec.nuaa.edu.cn/_upload/tpl/02/db/731/template731/pages/xtan/ClosedEyeDatabases.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Load packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "from ll_utils import *\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier, we will use leNet-5 model for detecting the eye status.\n",
    "\n",
    "#### What's leNet-5 model ?\n",
    "\n",
    "Yann LeCun, Leon Bottou, Yosuha Bengio and Patrick Haffner proposed a neural network architecture for handwritten and machine-printed character recognition in 1990’s which they called LeNet-5. The architecture is straightforward and simple to understand that’s why it is mostly used as a first step for teaching Convolutional Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet-5 architecture\n",
    "\n",
    "![LeNet-5 architecture](https://engmrk.com/wp-content/uploads/2018/09/LeNet_Original_Image.jpg)\n",
    "\n",
    "The LeNet-5 architecture consists of two sets of convolutional and average pooling layers, followed by a flattening convolutional layer, then two fully-connected layers and finally a softmax classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our leNet-5 model\n",
    "\n",
    "let's nuild our leNet-5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 24\n",
    "input_shape = (IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(AveragePooling2D())\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going load dataset and then we are going to train our model.\n",
    "\n",
    "for model training we will use: \n",
    "- [Binary crossentropy](https://papers.nips.cc/paper/8094-generalized-cross-entropy-loss-for-training-deep-neural-networks-with-noisy-labels.pdf) which is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). \n",
    "- [Adam optimization](https://arxiv.org/pdf/1412.6980.pdf) which is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3779 images belonging to 2 classes.\n",
      "Found 1067 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our model.\n",
    "\n",
    "we choose 20 training *epochs* and accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "118/118 [==============================] - 2s 15ms/step - loss: 0.6199 - accuracy: 0.6280 - val_loss: 0.4708 - val_accuracy: 0.7491\n",
      "Epoch 2/20\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.4226 - accuracy: 0.8089 - val_loss: 0.3804 - val_accuracy: 0.8396\n",
      "Epoch 3/20\n",
      "118/118 [==============================] - 2s 15ms/step - loss: 0.3301 - accuracy: 0.8695 - val_loss: 0.4827 - val_accuracy: 0.8715\n",
      "Epoch 4/20\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 0.2910 - accuracy: 0.8839 - val_loss: 0.4476 - val_accuracy: 0.8184\n",
      "Epoch 5/20\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.2789 - accuracy: 0.8860 - val_loss: 0.2691 - val_accuracy: 0.9043\n",
      "Epoch 6/20\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.2452 - accuracy: 0.9074 - val_loss: 0.1519 - val_accuracy: 0.8937\n",
      "Epoch 7/20\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.2192 - accuracy: 0.9178 - val_loss: 0.2473 - val_accuracy: 0.9217\n",
      "Epoch 8/20\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.2066 - accuracy: 0.9242 - val_loss: 0.2225 - val_accuracy: 0.9159\n",
      "Epoch 9/20\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.2035 - accuracy: 0.9218 - val_loss: 0.2241 - val_accuracy: 0.8995\n",
      "Epoch 10/20\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.1842 - accuracy: 0.9311 - val_loss: 0.0807 - val_accuracy: 0.9440\n",
      "Epoch 11/20\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.1780 - accuracy: 0.9341 - val_loss: 0.1810 - val_accuracy: 0.9362\n",
      "Epoch 12/20\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.1799 - accuracy: 0.9314 - val_loss: 0.1808 - val_accuracy: 0.9237\n",
      "Epoch 13/20\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.1563 - accuracy: 0.9410 - val_loss: 0.1370 - val_accuracy: 0.9304\n",
      "Epoch 14/20\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.1598 - accuracy: 0.9418 - val_loss: 0.0767 - val_accuracy: 0.9469\n",
      "Epoch 15/20\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.1570 - accuracy: 0.9424 - val_loss: 0.0823 - val_accuracy: 0.9285\n",
      "Epoch 16/20\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.1592 - accuracy: 0.9373 - val_loss: 0.1968 - val_accuracy: 0.9304\n",
      "Epoch 17/20\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.1416 - accuracy: 0.9442 - val_loss: 0.0294 - val_accuracy: 0.9256\n",
      "Epoch 18/20\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.1396 - accuracy: 0.9474 - val_loss: 0.1474 - val_accuracy: 0.9459\n",
      "Epoch 19/20\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.1350 - accuracy: 0.9477 - val_loss: 0.1183 - val_accuracy: 0.9411\n",
      "Epoch 20/20\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.1370 - accuracy: 0.9482 - val_loss: 0.0955 - val_accuracy: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2afe59b7d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = val_generator.n//val_generator.batch_size\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=20\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"save_model(model)\" is a function that you can find it\"s body in \"ll_utils.py\" file\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will test our model on some images.\n",
    "\n",
    "But first write our predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(filename):\n",
    "    print('Your image: ')\n",
    "    display(Image(filename= filename))\n",
    "    \n",
    "    # \"load_image(filename)\" is a function that you can find it\"s body in \"ll_utils.py\" file\n",
    "    image = load_image(filename)\n",
    "    prediction = model.predict(image)\n",
    "    if prediction < 0.1:\n",
    "        prediction = 'closed'\n",
    "    elif prediction > 0.9:\n",
    "        prediction = 'open'\n",
    "    else:\n",
    "        prediction = 'I am not sure'\n",
    "    \n",
    "    print(\"The eye status is \" + prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this predict function to test our model on some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your image: \n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAYABgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AM6aeW+uTGgJBPQc1v6XYR20LNMnzY6tXL69cAybFwTntWxaW7GFUikMT9Sy4yT+IrLu4PEMTnbfNICSACo4+tY40nXLi4LXK4APUkYP4V3Nmi5ZD95D1rQNuWGTyahlhwnI6V//2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eye status is closed\n"
     ]
    }
   ],
   "source": [
    "predict('dataset/test/test_03.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your image: \n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAYABgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AGT3cNzmKPd8nO4Z/pWNcavq0ZIgltQPSSdAcfi1XNN1u7cmO5hKvxypyD9ME1hXs97qWofY7aBlgGcIGIDEDq2Ov0qtplpq9/qP2WSH7NEgPmfJhT/9atq20+a1gaZJNyIeccDr2FaUVh9nldHjcEHKOpwQPSp5oY1iy08rcdC5Oaqvbzm2ZypjRuI0x1r/2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eye status is open\n"
     ]
    }
   ],
   "source": [
    "predict('dataset/test/test_01.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, It's good to do a recaps.\n",
    "\n",
    "- We built our model using leNet-5.\n",
    "- We trained it on Closed Eyes In The Wild (CEW) dataset.\n",
    "- We acheive an accuracy of **94.59%**\n",
    "- We test it on some random pictures.\n",
    "\n",
    "And below we have a model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 22, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 11, 11, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 16)          880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 85        \n",
      "=================================================================\n",
      "Total params: 42,029\n",
      "Trainable params: 42,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Liveliness Net make the face recognition system more secure by detecting non-real/fake faces.\n",
    "- In this notebook we handle with liveliness detector as a binary classification based on eye status(opened/closed).\n",
    "\n",
    "#### References:\n",
    "- [](https://towardsdatascience.com/real-time-face-liveness-detection-with-python-keras-and-opencv-c35dc70dafd3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
